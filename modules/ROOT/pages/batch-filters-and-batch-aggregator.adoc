= Using Batch Processing Scopes
ifndef::env-site,env-github[]
include::_attributes.adoc[]
endif::[]
:page-aliases: batch-aggregator-concept.adoc, filter-records-batch-faq.adoc

You can set up and configure Mule batch processing scopes to perform common batch processing use cases and to enhance the performance of batch jobs for cases in which the defaults require adjustment.

//TODO: NEEDS EDITING
== Filtering the Records to Process within a Batch Step Scope

You can apply one or more filters as attributes to any number of batch steps. +
Imagine a batch job whose first batch step checks if a Salesforce contact exists for a record, and a second batch step that updates each existing Salesforce contact with new information. You can apply a filter to the second batch step to ensure it only processes records that didn't fail during the first batch step. +
By having batch steps accept only some records for processing, you streamline the batch job so the Mule runtime engine can focus only on the relevant data for a particular batch step.

A batch step uses two attributes to filter records:

* acceptExpression
* acceptPolicy

Each batch step can accept one acceptExpression and one acceptPolicy attributes to filter records.

Use the acceptExpression attribute to process only records that evaluate to true; if the record evaluates to false, the batch step skips the record and sends it to the next one. In other words, the records with an accept expression that resolves to false are the ones that Mule filters _out_.

The example below filters out all records where the age is less than 21; the batch step does not process those records.

[source,xml,linenums]
----
<batch:job jobName="batchJob">
	<batch:process-records >
		<batch:step name="adultsOnlyStep" acceptExpression="#[payload.age > 21]">
			...
		</batch:step>
	</batch:process-records>
 </batch:job>
----

Use the acceptPolicy attribute from batch step to process only the records which, relative to the value of the accept policy attribute, evaluate to true. Refer to the table below for a list of the available values for the accept policy.

[%header,cols="25a,75a"]
|===
|Accept Policy |When evaluates to TRUE
|NO_FAILURES |_Default_ +
Batch step processes only those records that _succeeded_ to process in all preceding steps.
|ONLY_FAILURES |Batch step processes only those records that _failed_ to process in a preceding batch step.
|ALL |Batch step processes all records, regardless of whether they failed to process in a preceding batch step.
|===

If you don't apply filters to a batch step, the batch processes only those records that _succeeded_ to process in all preceding steps. In other words, the default Accept Policy applied to all batch steps is NO_FAILURES.

The example below illustrates the second batch step in a batch job that processes only those records that failed to process during the preceding step. In the first batch step, the runtime checked each record to see if it had an existing Salesforce contact; the second batch step, which creates a contact for each record, processes only the failed records (that is, records that failed to have an existing account).

[source,xml,linenums]
----
<batch:job jobName="batchJob">
	<batch:process-records >
		<batch:step name="batchStep1">
			...
		</batch:step>
		<batch:step name="batchStep2" accept-policy="ONLY_FAILURES">
			...
		</batch:step>
	</batch:process-records>
 </batch:job>
----

Each batch job has a `maxFailedRecords` attribute that controls how many failed records you are willing to accept for a batch job. +
When a batch job instance exceeds its `maxFailedRecords` value, regardless of the filter set on the batch step, the step does not process any records and pushes the failed batch job instance to the On Complete phase. +
See xref:batch-error-handling-faq.adoc[Handling Errors During Batch Job] for more information.

//TODO: WHAT WE'RE MISSING IS A REFERENCE THAT DESCRIBES FILTERS
Filter characteristics:

* Batch filters only apply to batch steps which, in turn, are only usable within the batch process phase of a batch job. You cannot apply filters with the Input or On Complete phases.
* If you apply no filters to a batch step, the batch processes only those records which _succeeded_ to process in all preceding steps. In other words, the default Accept Policy applied to all batch steps is NO_FAILURES.
* When a batch job instance exceeds its `max-failed-records` value, regardless of the filter set on the batch step, the step does not process any records and pushes the failed batch job instance to the On Complete phase.
* Where you apply both types of filters, Mule evaluates them in the following order:
+
. Accept Policy
. Accept Expression


[[aggregator_bulk_update]]
== Performing Bulk Uploads from a Batch Aggregator Scope

Aggregation is useful for bulk uploading multiple records within an array to an external server, such as a database server. Within the Batch Aggregator scope, you can add an operation, such as a bulk update, insert, or upsert operation, to load multiple records to a server with a single execution of an operation, instead of running an operation separately on each record.

You can process records in separate arrays of a fixed size or stream a single array of records from the batch job instance. For example, you can configure a Bulk Update operation in the Anypoint Connector for Database (Database Connector) to update 200 processed records on a database server. Alternatively, you can stream all the records in the instance to the server. For information about configuring bulk operations for Database Connector, see xref:db-connector::database-execute-bulk.adoc[].

The following example bulk updates a database in separate arrays of 200 records per update:

[source,xml,linenums]
----
<batch:job jobName="batchJob">
  <batch:process-records >
    <batch:step name="batchStep">
      <batch:aggregator size="200">
        <db:bulk-update config-ref="Database_Config">
      </batch:aggregator>
    </batch:step>
  </batch:process-records>
</batch:job>
----

The following example streams updates to a database:

[source,xml,linenums]
----
<batch:job jobName="batchJob">
	<batch:process-records >
		<batch:step name="batchStep">
		  <batch:aggregator streaming="true">
		    <db:bulk-update config-ref="Database_Config">
		  </batch:aggregator>
		</batch:step>
	</batch:process-records>
</batch:job>
----

To learn more about processing within the Batch Aggregation scope and how aggregation differs from processing within the Batch Step scope, see xref:batch-processing-concept.adoc#phase_process[Process Phase].

[[aggregator_modify_records]]
== Modifying Records within a Batch Aggregator

You can modify records within the Batch Aggregator scope, just as you can modify them with processors in the Batch Step scope. The modifications can take place sequentially or through random access to specific records.

=== Sequential Processing within a Batch Aggregator

The following example performs sequential access to records. Using Groovy within a xref:scripting-module::index.adoc[Scripting module], the example modifies the payload of each record from the xref:for-each-scope-concept.adoc[For Each] scope's iteration and creates a variable for each collected record.

[source,xml,linenums]
----
<batch:job jobName="batchJob">
	<batch:process-records>
		<batch:step name="batchStep">
			<batch:aggregator doc:name="batchAggregator" size="10">
				<foreach doc:name="For Each">
					<script:execute engine="groovy">
			    	<script:code>
			        		vars['marco'] = 'polo'
					        vars['record'].payload = 'hello'
			    	</script:code>
					</script:execute>
				</foreach>
			</batch:aggregator>
		</batch:step>
	</batch:process-records>
</batch:job>
----

//TODO: NEED TO EXPLAIN THE FOLLOWING BULLET POINTS.
Sequential access to records assumes the following:

. The aggregator size matches the number of aggregated records.
. There is a direct correlation between the aggregated records and the items in the list.

=== Randomly Accessing Records within a Batch Aggregator

//TODO: HOW WOULD ANYONE KNOW THE "iteration number" of the record?
//Is that number the index of the record in the list, or what?
You can also use the For Each scope when randomly accessing records by their iteration number. For Each exposes a `records` variable, which is an immutable list that For Each uses to keep track of the iteration. You can use this variable to randomly access records in the list from the Batch Aggregator scope.

To demonstrate random access when using fixed-size aggregation, the following example specifies the index of a record in the For Each list of records. Instead of sequentially accessing each record, using the `records` variable with an index of a record selects a single record from the list. The example uses the Scripting module to modify the payload of the selected record and create a variable for that record:

[source,xml,linenums]
----
<batch:job jobName="batchJob">
	<batch:process-records>
		<batch:step name="batchStep">
			<batch:aggregator doc:name="batchAggregator" size="10">
				<foreach doc:name="For Each">
					<script:execute engine="groovy">
			    	<script:code>
			        	records[0].vars['marco'] = 'polo'
				        records[0].vars['record'].payload = 'hello'
			    	</script:code>
					</script:execute>
				</foreach>
			</batch:aggregator>
		</batch:step>
	</batch:process-records>
</batch:job>
----

You can configure a Batch Aggregator scope to stream its content. Setting aggregator to stream the records (`streaming="true"`) enables you to process an array of all the records in the job instance without running out of memory, no matter how many or how large the records are. For example, if you need to write millions of records to a CSV file, you can process the records as a streaming batch aggregator.

[source,xml,linenums]
----
<batch:job jobName="batchJob">
	<batch:process-records >
		<batch:step name="batchStep">
			<batch:aggregator streaming="true">
				<file:write path="reallyLarge.csv">
					<file:content><![CDATA[%dw 2.0
						...

					}]]></file:content>
			</batch:aggregator>
		</batch:step>
	</batch:process-records>
</batch:job>
----

You can modify the records in the stream. However, only sequential access to records is possible when streaming within the aggregator. Random access is not supported when streaming because record payloads for random access are exposed as an `ImmutableList`. Streaming through an aggregator requires access to the entire set of records. Without a fixed size. Mule cannot guarantee that all records will fit in memory.

[source,xml,linenums]
----
<batch:job jobName="batchJob">
	<batch:process-records>
		<batch:step name="batchStep">
			<batch:aggregator doc:name="batchAggregator" streaming="true">
				<foreach doc:name="For Each">
					<script:execute engine="groovy">
						<script:code>
							vars['marco'] = 'polo'
							vars['record'].payload = 'foo'
						</script:code>
					</script:execute>
				</foreach>
			</batch:aggregator>
		</batch:step>
	</batch:process-records>
</batch:job>
----

To learn more about processing within the Batch Aggregation scope and how aggregation differs from processing within the Batch Step scope, see xref:batch-processing-concept.adoc#phase_process[Process Phase].

Streaming Tips:

* Streaming from SaaS providers: In general, you are unlikely to use batch streaming when sending data through an Anypoint Connector to a SaaS provider, such as Salesforce, because SaaS providers often have restrictions on accepting streaming input.

* Batch streaming is often useful in batch processing when writing to a file such as CSV, JSON, or XML.

* Batch streaming and performance: Batch streaming affects the performance of your application because the array of records is stored in RAM, which can slow the pace at which transactions process. Though performance slows, the trade-off of streaming data can warrant using it in your implementation.

* Batch streaming and access to items: The biggest drawback to using batch streaming is the limitation on access to the items in the output. A fixed-size commit provides an unmodifiable list of records,
//TODO: IS "an unmodifiable" correct here or is it "a modifiable"
//TODO: What do we mean by "commit" here? as in "fixed-size commit" and "streaming
//commit". Should we use that term? Could we use a different term if we wanted? If ////so, what term?
which enables you to access and process the items iteratively using one or more processors within the aggregator. A streaming commit is a one-read, forward-only iterator, which means that only one processor in the aggregator can read and process the input to the aggregator. However, you can use a scope like For Each with multiple processors within it because For Each maintains its own list of records.

//TODO: WHAT WE'RE MISSING IS A REFERENCE THAT DESCRIBES EACH SCOPES
Batch aggregator characteristics:

* The Batch Aggregator scope is an optional component that you can add to a Batch Step component. Only one aggregator per step is allowed. Aggregation is available only in the Process phase of a batch job instance and in no other phase.
//TODO: WHAT DOES THIS MEAN? "can only wrap the final element ..."
* An aggregator can only wrap the final element within the batch step in which it resides.
* Many connectors provide operations that can handle record-level errors without causing an entire batch aggregation process to fail.
//TODO: Which Salesforce and which NetSuite connectors?
An example is `upsert` in the Salesforce and NetSuite connectors.
+
At runtime, the connectors with these operations keep track of which records that the target resource accepts successfully and which fail to upsert. Rather than failing a complete group of records, the connector upserts as many records as it can and tracks any failures for notification.
+
//TODO: TELLING PEOPLE TO CHECK THE CONNECTOR DOCS ISN'T A GREAT OPTION.
To make sure that the connector you are using supports record-level errors, check the connector's documentation.
//TODO: NEED TO EDIT AND POINT TO INFORMATION ABOUT TRANSACTIONS
//OR EXPLAIN THAT CONCEPT MORE HERE
* The Batch Aggregator scope does not support job-instance-wide transactions. Within a Batch Step scope, you can define a transaction that processes each record in a separate transaction. Think of it as a step within a step.
+
Any transaction started by a Batch Step scope ends before the Batch Aggregator scope starts processing. A transaction cannot cross the boundary these scopes, so they cannot share a transaction.

NOTE::
When using an aggregator, do not attempt to process Guava data types, such as `ImmutableMap`. Instead, use a Java Map to avoid serialization issues.

//TODO: NEEDS EDITING
== Preserving the MIME types of the Aggregated Records

_Introduced in Mule 4.3_

Aggregated records are passed into the aggregator as an array containing each record’s payload. However, the MIME types associated with those payloads are not preserved by default. You can preserve record’s MIME types by specifying the `preserveMimeTypes` attribute in a batch aggregator.

As an example, consider the following JSON document:

[source,json,linenums]
----
[
	{
		"name": "Tony Stark",
		"alias": "Iron Man",
		"status": "DEAD"
	},
	{
		"name": "Steve Rodgers",
		"alias": "Captain America",
		"status": "RETIRED"
	},
	{
		"name": "Peter Parker",
		"alias": "SpiderMan",
		"status": "FUGITIVE"
	}
]
----

Suppose you fed this document into the following job:

[source,xml,linenums]
----
<batch:job name="avengersLogger">
	<batch:process-records>
		<batch:step name="log">
			<batch:aggregator size="10">
				<foreach>
					<logger message="Agent #[payload.alias] is #[payload.status]" />
				</foreach>
			</batch:aggregator>
		</batch:step>
	</batch:process-records>
</batch:job>
----

The batch engine splits the input JSON array into individual records, which means that the aggregator block receives an array with three elements. The first one of them is:

[source,json,linenums]
----
{
 "name": "Tony Stark",
 "alias": "Iron Man",
 "status": "DEAD"
}
----

However, when the logger element attempts to evaluate the `#[payload.alias]` expression, it results in an error similar to the following:

[source,text,linenums]
----
********************************************************************************
Message               : "You called the function 'Value Selector' with these arguments:
  1: Binary ("ewogICJmaXJzdE5hbWUiOiAiUmFtIiwKICAibGFzdE5hbWUiOiAiUmFtMSIsCiAgImFkZHJlc3Mi...)
  2: Name ("alias")

But it expects one of these combinations:
  (Array, Name)
  (Array, String)
  (Date, Name)
  (DateTime, Name)
  (LocalDateTime, Name)
  (LocalTime, Name)
  (Object, Name)
  (Object, String)
  (Period, Name)
  (Time, Name)

5|                                         name: payload.alias,
                                                 ^^^^^^^^^^^^^
----

The previous error occurs because MIME types are not preserved by default, and therefore Mule doesn’t know that this record is actually a JSON. You can fix this by specifying the `preserveMimeTypes` attribute in the batch aggregator:

[source,xml,linenums]
----
<batch:aggregator size="10" preserveMimeTypes="true">
	<foreach>
	   <logger message="Agent #[payload.alias] is #[payload.status]" />
	</foreach>
</batch:aggregator>
----

By setting this attribute, Mule automatically maintains each record's media type and knows that the payload actually represents a JSON document.

//TODO: NEEDS EDITING
== Changing the Record Block Size

In a traditional online processing model, each request is usually mapped to a worker thread. Regardless of the processing type (either synchronous, asynchronous, one-way, request-response or even if the requests are temporarily buffered before being processed), servers usually end up in a 1:1 relationship between a request and a running thread. +
When it comes to a batch job, all records are first stored in a persistent queue before the Process phase begins, so that the traditional threading model wouldn't apply.

To improve performance, the runtime queues and schedules batch records in blocks
of up to 100 records per thread. This behavior reduces the number of I/O requests
and improves an operation's load. Batch jobs use the Mule runtime engine's thread pools, so
there is no default for the job. Each thread iterates through that block to
process each record, and then each block is queued back, and the process continues.

Consider having 1 million records to place in a queue for a 3-step batch job. At least three million I/O operations occur as the Mule runtime engine takes and requests each record as they move through the job's phases. +
Performance requires having enough available memory to process the threads in parallel, which means moving the records from persistent storage into RAM. The larger your records and their quantity, the more available memory you need for batch processing.

Although the standard model of up to 100 records per thread in the batch job works for most use cases, consider three use cases where you need to increase or decrease the block size:

* Assume you have 200 records to process through a batch job. With the default 100-record block size, Mule can only process two records in parallel at a time. If a batch job has fewer than 101 records to process, then processing becomes sequential. If you need to process heavy payloads, then queueing a hundred records demands a large amount of working memory.
* Consider a batch job that needs to process images, and an average image size of 3 MB. In this case, Mule processes 100-record blocks with payloads of 3 MB in each thread. Hence, your default threading-profile setting would require a large amount of working memory just to keep the blocks in the queue. In this case, set a lower block size to distribute each payload through more jobs and lessen the load on your available memory.
* Suppose you have 5 million records with payloads so small that you can fit blocks of 500 records in your memory without problems. Setting a larger block size improves your batch job time without sacrificing working memory load.

To take full advantage of this feature, you must understand how the block sizes affect your batch job. Running comparative tests with different values and testing performance helps you find an optimum block size before moving this change into production.

Remember that modifying the batch block size is optional. If you apply no changes, the default value is 100 records per block.

You set the size through the Batch Job scope, for example:

[source,xml,linenums]
----
<batch:job jobName="atch_Job" blockSize="200">
  ...
</batch:job>
----

//TODO: NEEDS EDITING AND ELABORATION
== Setting a Max Concurrency Limit on Batch Job Instances

The Max Concurrency (`maxConcurrency`) property limits the number of blocks that the batch job can process concurrently. The records inside the block are processed sequentially.

You can configure the `maxConcurrency` property as in the following example:

[source,xml,linenums]
----
<batch:job jobName="test-batch" maxConcurrency="${batch.max.concurrency}">
  ...
</batch:job>
----

By default, the Batch Job component limits the max concurrency to twice the number of available cores. The capacity of the system running the Mule instance also limits concurrency.

== See Also

* xref:batch-processing-concept.adoc[]
* xref:batch-job-instance-id.adoc[]
* xref:tuning-backpressure-maxconcurrency.adoc[]
